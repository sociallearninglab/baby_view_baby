<!DOCTYPE html>
<html>
<head>
    <title>Baby Mirror Self-Recognition</title>
    <!-- Load jsPsych with full URLs for GitHub Pages -->
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/jspsych.js"></script>
    <link href="https://sociallearninglab.github.io/baby_view_baby/jspsych/jspsych.css" rel="stylesheet" type="text/css">
    
    <!-- Load required plugins -->
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/plugin-html-keyboard-response.js"></script>
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/plugin-video-keyboard-response.js"></script>
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/plugin-preload.js"></script>
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/plugin-html-button-response.js"></script>
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/extension-webgazer.js"></script>
    
    <!-- Load WebGazer -->
    <script src="https://webgazer.cs.brown.edu/webgazer.js"></script>
    
    <!-- Add custom styles -->
    <style>
        #webcam-container {
            position: fixed;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            object-fit: cover;
        }
        #webcam {
            width: 100%;
            height: 100%;
            transform: scaleX(-1);
        }
        .calibration-point {
            position: absolute;
            width: 25px;
            height: 25px;
            background: red;
            border-radius: 50%;
            transform: translate(-50%, -50%);
            animation: pulse 1s infinite;
        }
        @keyframes pulse {
            0% { transform: translate(-50%, -50%) scale(1); }
            50% { transform: translate(-50%, -50%) scale(1.5); }
            100% { transform: translate(-50%, -50%) scale(1); }
        }
    </style>
</head>
<body>
    <div id="jspsych-target"></div>
    <audio id="chime" src="https://github.com/sociallearninglab/baby_view_baby/raw/refs/heads/main/mp3/chime.mp3"></audio>
    <audio id="song" src="https://github.com/sociallearninglab/baby_view_baby/raw/refs/heads/main/mp3/song.mp3"></audio>

    <script>
        // Get participant ID from URL parameters (sent by Lookit)
        const urlParams = new URLSearchParams(window.location.search);
        const participantId = urlParams.get('participantId') || 'unknown';
        let gazeData = [];
        let mediaRecorder;
        const recordedChunks = [];

        // Initialize jsPsych
        const jsPsych = initJsPsych({
            on_finish: function() {
                // Prepare metadata to send back to Lookit
                const experimentData = {
                    participantId: participantId,
                    timestamp: new Date().toISOString(),
                    experimentComplete: true,
                    // Send just a sample of the gaze data (first 20 points)
                    gazeSample: gazeData.slice(0, 20),
                    // Add metadata about recording
                    recordingInfo: {
                        durationMs: 120000,
                        dataPoints: gazeData.length,
                        complete: true
                    }
                };

                // Send a message back to Lookit
                window.opener.postMessage({
                    type: 'taskComplete',
                    data: experimentData
                }, '*');
                
                // Log completion to console but don't download anything
                console.log("Experiment completed for participant: " + participantId);
                console.log("Data collected: " + gazeData.length + " gaze data points");
            }
        });

        // Welcome
        const welcome = {
            type: jsPsychHtmlButtonResponse,
            stimulus: `
                <h1>Welcome!</h1>
                <p>Thank you for participating in our study about how babies view faces!</p>
            `,
            choices: ['Begin'],
        };

        // Initialize WebGazer
        const initWebGazer = {
            type: jsPsychHtmlButtonResponse,
            stimulus: `
                <h2>Eye Tracking Setup</h2>
                <p>We'll set up eye tracking with automatic calibration. Click begin when ready.</p>
            `,
            choices: ['Begin'],
            on_load: function() {
                webgazer.begin();
                webgazer.showVideoPreview(false);
                webgazer.showPredictionPoints(false);
            }
        };

        // Calibration setup
        const calibrationInstructions = {
            type: jsPsychHtmlButtonResponse,
            stimulus: `
                <h2>Calibration</h2>
                <p>Now we'll show your child some interesting patterns to help us track where they're looking.</p>
            `,
            choices: ['Start Calibration'],
        };

        // Calibration sequence
        const calibrationTrial = {
            type: jsPsychHtmlKeyboardResponse,
            stimulus: `
                <div style="position: relative; width: 100vw; height: 100vh;">
                    <div id="calibration-point" class="calibration-point"></div>
                </div>
            `,
            choices: "NO_KEYS",
            trial_duration: 10000, // 2 seconds Ã— 5 points
            on_load: function() {
                const point = document.getElementById('calibration-point');
                const chime = document.getElementById('chime');
                const positions = [
                    { left: '50%', top: '50%' },  // center
                    { left: '20%', top: '50%' },  // left
                    { left: '80%', top: '50%' },  // right
                    { left: '50%', top: '20%' },  // top
                    { left: '50%', top: '80%' }   // bottom
                ];
                let currentPosition = 0;

                function movePoint() {
                    if (currentPosition < positions.length) {
                        point.style.left = positions[currentPosition].left;
                        point.style.top = positions[currentPosition].top;
                        // Play chime sound
                        chime.currentTime = 0;  // Reset the audio to start
                        chime.play().catch(e => console.log("Audio play failed:", e));
                        currentPosition++;
                        setTimeout(movePoint, 2000); // Move every 2 seconds
                    }
                }

                movePoint();
            }
        };

        // Mirror trial
        const mirrorTrial = {
            type: jsPsychHtmlKeyboardResponse,
            stimulus: `
                <div id="webcam-container">
                    <video id="webcam" autoplay playsinline></video>
                </div>
            `,
            choices: [' '], // Allow spacebar to end trial
            trial_duration: 120000, // 2 minutes
            on_load: function() {
                // Get the audio element and play soundtrack
                const song = document.getElementById('song');
                song.currentTime = 0;  // Reset to start
                song.play().catch(e => console.log("Audio play failed:", e)); // Play the song

                // Start eye tracking without showing the prediction
                webgazer.setGazeListener(function(data, elapsedTime) {
                    if (data == null) return;
                    
                    // Store gaze data without showing the dot
                    gazeData.push({
                        timestamp: Date.now(),
                        x: data.x,
                        y: data.y
                    });
                }).begin();

                // Start video recording (but don't save it)
                navigator.mediaDevices.getUserMedia({ 
                    video: { width: { ideal: 1920 }, height: { ideal: 1080 } },
                    audio: false
                })
                .then(function(stream) {
                    const video = document.getElementById('webcam');
                    video.srcObject = stream;

                    // We still set up the recorder for consistency, but don't save data
                    mediaRecorder = new MediaRecorder(stream);
                    mediaRecorder.start();

                    // Stop recording after 2 minutes or if spacebar pressed
                    setTimeout(() => {
                        if (mediaRecorder.state !== 'inactive') {
                            mediaRecorder.stop();
                        }
                        stream.getTracks().forEach(track => track.stop());

                        song.pause();  // Stop the song when trial ends
                        song.currentTime = 0; // Reset song for potential reuse
                    }, 120000); // 2 minutes
                })
                .catch(function(err) {
                    console.error("Error accessing webcam:", err);
                    alert('Error accessing webcam. Please make sure you have granted camera permissions.');
                });
            },
            on_finish: function() {
                // Make sure song stops if trial ended early (from spacebar)
                const song = document.getElementById('song');
                song.pause();
                song.currentTime = 0;
            }
        };

        // Exit
        const exitSurvey = {
            type: jsPsychHtmlButtonResponse,
            stimulus: `
                <h2>Thank you for participating!</h2>
                <p>We are studying how babies respond to seeing themselves.</p>
            `,
            choices: ['Complete Study'],
        };

        // timeline
        const timeline = [
            welcome,
            initWebGazer,
            calibrationInstructions,
            calibrationTrial,
            mirrorTrial,
            exitSurvey
        ];

        // Start the experiment
        jsPsych.run(timeline);
    </script>
</body>
</html>