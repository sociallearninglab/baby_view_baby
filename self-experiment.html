<!DOCTYPE html>
<html>
<head>
    <title>Baby Mirror Self-Recognition</title>
    <!-- Load jsPsych with correct GitHub Pages paths -->
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/jspsych.js"></script>
    <link href="https://sociallearninglab.github.io/baby_view_baby/jspsych/jspsych.css" rel="stylesheet" type="text/css">
    
    <!-- Load required plugins -->
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/plugin-html-keyboard-response.js"></script>
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/plugin-video-keyboard-response.js"></script>
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/plugin-preload.js"></script>
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/plugin-html-button-response.js"></script>
    <script src="https://sociallearninglab.github.io/baby_view_baby/jspsych/extension-webgazer.js"></script>
    
    <!-- Load WebGazer -->
    <script src="https://webgazer.cs.brown.edu/webgazer.js"></script>
    
    <!-- Add custom styles -->
    <style>
        #webcam-container {
            width: 960px;
            height: 720px;
            margin: 0 auto;
            border: 2px solid #ccc;
            position: relative;
        }
        #webcam {
            width: 100%;
            height: 100%;
            transform: scaleX(-1);
        }
        .calibration-point {
            position: absolute;
            width: 25px;
            height: 25px;
            background: red;
            border-radius: 50%;
            transform: translate(-50%, -50%);
            animation: pulse 1s infinite;
        }
        @keyframes pulse {
            0% { transform: translate(-50%, -50%) scale(1); }
            50% { transform: translate(-50%, -50%) scale(1.5); }
            100% { transform: translate(-50%, -50%) scale(1); }
        }
    </style>
</head>
<body>
    <div id="jspsych-target"></div>
    <audio id="chime" src="https://github.com/sociallearninglab/baby_view_baby/raw/refs/heads/main/mp3/chime.mp3"></audio>
    <audio id="song" src="https://github.com/sociallearninglab/baby_view_baby/raw/refs/heads/main/mp3/song.mp3"></audio>

    <script>
        // Get participant ID from URL parameters (sent by Lookit)
        const urlParams = new URLSearchParams(window.location.search);
        const participantId = urlParams.get('participantId') || 'unknown';
        let gazeData = [];
        let mediaRecorder;
        const recordedChunks = [];

        // Initialize jsPsych
        const jsPsych = initJsPsych({
            on_finish: function() {
                // Prepare data to send back to Lookit
                const experimentData = {
                    participantId: participantId,
                    timestamp: new Date().toISOString(),
                    gazeData: gazeData,
                    videoData: recordedChunks
                };

                // Send data back to Lookit
                window.opener.postMessage({
                    type: 'taskComplete',
                    data: experimentData
                }, '*');
            }
        });

        // Welcome
        const welcome = {
            type: jsPsychHtmlButtonResponse,
            stimulus: `
                <h1>Welcome!</h1>
                <p>Thank you for participating in our study about how babies view faces!</p>
            `,
            choices: ['Begin'],
        };

        // Initialize WebGazer
        const initWebGazer = {
            type: jsPsychHtmlButtonResponse,
            stimulus: `
                <h2>Eye Tracking Setup</h2>
                <p>We'll set up eye tracking with automatic calibration. Click begin when ready.</p>
            `,
            choices: ['Begin'],
            on_load: function() {
                webgazer.begin();
                webgazer.showVideoPreview(false);
                webgazer.showPredictionPoints(false);
            }
        };

        // Calibration setup
        const calibrationInstructions = {
            type: jsPsychHtmlButtonResponse,
            stimulus: `
                <h2>Calibration</h2>
                <p>Now we'll show your child some interesting patterns to help us track where they're looking.</p>
            `,
            choices: ['Start Calibration'],
        };

        // Calibration sequence
        const calibrationTrial = {
            type: jsPsychHtmlKeyboardResponse,
            stimulus: `
                <div style="position: relative; width: 100vw; height: 100vh;">
                    <div id="calibration-point" class="calibration-point"></div>
                </div>
            `,
            choices: "NO_KEYS",
            trial_duration: 10000, // 2 seconds Ã— 5 points
            on_load: function() {
                const point = document.getElementById('calibration-point');
                const chime = document.getElementById('chime');
                const positions = [
                    { left: '50%', top: '50%' },  // center
                    { left: '20%', top: '50%' },  // left
                    { left: '80%', top: '50%' },  // right
                    { left: '50%', top: '20%' },  // top
                    { left: '50%', top: '80%' }   // bottom
                ];
                let currentPosition = 0;

                function movePoint() {
                    if (currentPosition < positions.length) {
                        point.style.left = positions[currentPosition].left;
                        point.style.top = positions[currentPosition].top;
                        // Play chime sound
                        chime.currentTime = 0;  // Reset the audio to start
                        chime.play().catch(e => console.log("Audio play failed:", e));
                        currentPosition++;
                        setTimeout(movePoint, 2000); // Move every 2
                    }
                }

                movePoint();
            }
        };

        // Mirror trial
        const mirrorTrial = {
            type: jsPsychHtmlKeyboardResponse,
            stimulus: `
                <div id="webcam-container">
                    <video id="webcam" autoplay playsinline></video>
                </div>
            `,
            choices: "NO_KEYS",
            trial_duration: 15000,
            on_load: function() {
                // Get the audio element
                const song = document.getElementById('song');
                song.currentTime = 0;  // Reset to start
                song.play().catch(e => console.log("Audio play failed:", e)); // Play the song

                // Start eye tracking without showing the prediction
                webgazer.setGazeListener(function(data, elapsedTime) {
                    if (data == null) return;
                    
                    // Store gaze data without showing the dot
                    gazeData.push({
                        timestamp: Date.now(),
                        x: data.x,
                        y: data.y
                    });
                }).begin();

                // Start video recording
                navigator.mediaDevices.getUserMedia({ 
                    video: { width: 960, height: 720 },
                    audio: false
                })
                .then(function(stream) {
                    const video = document.getElementById('webcam');
                    video.srcObject = stream;

                    mediaRecorder = new MediaRecorder(stream, {
                        mimeType: 'video/webm;codecs=vp9'
                    });

                    mediaRecorder.ondataavailable = function(event) {
                        if (event.data.size > 0) {
                            recordedChunks.push(event.data);
                        }
                    };

                    mediaRecorder.onstop = function() {
                        const blob = new Blob(recordedChunks, { type: 'video/webm' });
                        const url = URL.createObjectURL(blob);
                        const a = document.createElement('a');
                        a.href = url;
                        a.download = `mirror_recording_${Date.now()}.webm`;
                        document.body.appendChild(a);
                        a.click();
                        document.body.removeChild(a);
                        URL.revokeObjectURL(url);
                    };

                    mediaRecorder.start();

                    setTimeout(() => {
                        if (mediaRecorder.state !== 'inactive') {
                            mediaRecorder.stop();
                        }
                        stream.getTracks().forEach(track => track.stop());

                        song.pause();  // Stop the song when trial ends
                        song.currentTime = 0; // Reset song for potential reuse
                    }, 600000);
                })
                .catch(function(err) {
                    console.error("Error accessing webcam:", err);
                    alert('Error accessing webcam. Please make sure you have granted camera permissions.');
                });
            }
        };

        // Exit
        const exitSurvey = {
            type: jsPsychHtmlButtonResponse,
            stimulus: `
                <h2>Thank you for participating!</h2>
                <p>We are studying how babies respond to seeing themselves.</p>
            `,
            choices: ['Complete Study'],
        };

        // timeline
        const timeline = [
            welcome,
            initWebGazer,
            calibrationInstructions,
            calibrationTrial,
            mirrorTrial,
            exitSurvey
        ];

        // Start the experiment
        jsPsych.run(timeline);
    </script>
</body>
</html>